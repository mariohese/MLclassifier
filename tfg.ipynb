{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, TweetTokenizer, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ne_chunk, pos_tag\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier #For Classification\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from nltk.collocations import *\n",
    "\n",
    "from preprocessor import preProcessSerie\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import pickle\n",
    "\n",
    "#Para el support vector machine:\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import cross_val_score, KFold, StratifiedKFold\n",
    "\n",
    "#Optimizaci√≥n\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "\n",
    "#Para kNN\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Importar librer√≠a del gsi\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "#%run plot_learning_curve\n",
    "\n",
    "\n",
    "tknzr = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles = True)\n",
    "tknzrwhu = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles = False)\n",
    "stemmer=SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "#Creo una variable que contenga todas las palabras de los tweets\n",
    "#para poder filtrar posteriormente las palabras m√°s frecuentes.\n",
    "listwords = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Como la lemmatizer requiere un pos_tag, crearemos este m√©todo para\n",
    "#d√°rselo y posteriormente haremos lemmatizer en cada tweet\n",
    "def lemmaSentence(sentence):\n",
    "    result = \"\"\n",
    "    wnl = WordNetLemmatizer()\n",
    "    poscategory = pos_tag(tknzr.tokenize(sentence))\n",
    "    for word, tag in poscategory:\n",
    "        wntag = tag[0].lower()\n",
    "        wntag = wntag if wntag in ['a','r','n','v'] else None\n",
    "        if not wntag:\n",
    "            lemma = word\n",
    "        else:\n",
    "            lemma = wnl.lemmatize(word, wntag)\n",
    "        result = result + \" \" + lemma\n",
    "#    print(result)\n",
    "    return result\n",
    "\n",
    "#Como la lemmatizer requiere un pos_tag, crearemos este m√©todo para\n",
    "#d√°rselo y posteriormente lemmatizar las palabras\n",
    "def lemmaWord(wordsinlist):\n",
    "    i = 0\n",
    "    wnl = WordNetLemmatizer()\n",
    "    poscategory = pos_tag(wordsinlist)\n",
    "    for word, tag in poscategory:\n",
    "        wntag = tag[0].lower()\n",
    "        wntag = wntag if wntag in ['a','r','n','v'] else None\n",
    "        if not wntag:\n",
    "            lemma = word\n",
    "        else:\n",
    "            lemma = wnl.lemmatize(word, wntag)\n",
    "        wordsinlist[i] = str(lemma)\n",
    "        i = i + 1\n",
    "    return wordsinlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creo un preprocesador que utilizar√© posteriormente\n",
    "#En √©l trato de eliminar las urls, signos de puntuaci√≥n y stopwords\n",
    "#as√≠ como palabras como RT, posteriormente hago stemming\n",
    "def preProcess(serie):\n",
    "    listatweets = []\n",
    "    for line in serie:\n",
    "        frase = \"\"\n",
    "        l = \"\"\n",
    "        #try:\n",
    "            #l = bing(line, dst='en')\n",
    "        #except TranslateError:\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "        line = str(emoji_pattern.sub(r'', str(line))) # no emoji\n",
    "        l = line\n",
    "        pal = tknzrwhu.tokenize(str(l))\n",
    "        urls = re.compile(r'.http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "        line = urls.sub('', str(line))\n",
    "        pr = [\"rt\",\"@\",\"http\",\"https\",\"'s\",'...', 'english', 'translation','):', '. .', '..',\n",
    "         '√†', 'ÿü', 'Ÿà', '‡§Ç', '‡§ï', '‡§ó', '‡§ú', '‡§°', '‡§§', '‡§•', '‡§¶', '‡§®', '‡§™', '‡§¨', '‡§≠', '‡§Æ', '‚Ä¶',\n",
    "          '‡§Ø', '‡§∞', '‡§≤', '‡§µ', '‡§∂', '‡§∏', '‡§π', '‡§æ', '‡§ø', '‡•Ä', '‡•Å', '‡•Ç', '‡•á', '‡•à', '‡•ã',\n",
    "          '‡•ç', '‡¥æ', '‡¥ø', '‡µç', '‡∏ó', '‡∏û', '‡∏±', '‡∏¥', '‡∏µ', '‡∏∑', '‡∏π', '‡πá', '‡πà', '‡πâ', '‡πå', '‚Äì', '‚Äî', '‚Äò',\n",
    "          '‚Äô', '‚Äú', '‚Äù', '„ÄÅ', '„ÄÇ', '„Äå', '„Äç', '„Äê', '„Äë', 'Ô∑∫', 'Ô∏è', 'ÔºÅ', 'Ôºà', 'Ôºâ', 'üèª', 'üì∞', '',\n",
    "          'üì∑', 'üî•', 'üî¥', 'üòÇ']\n",
    "        ht = re.compile(r'http.')\n",
    "        bar = re.compile(r'//*')\n",
    "        punctuation = set(string.punctuation)\n",
    "        stoplist = stopwords.words('english')\n",
    "        #d = enchant.Dict(\"en_GB\")\n",
    "        pal = [str(i) for i in pal if i not in pr \n",
    "            if i not in stoplist if i not in punctuation\n",
    "            if not bar.search(i) if not ht.search(i)\n",
    "            if not i.isdigit()]\n",
    "        #Dentro tenia tb if d.check(str(i))\n",
    "        [listwords.append(lemmaSentence(i)) for i in pal]\n",
    "        for p in pal:\n",
    "            frase = frase + \" \" + p\n",
    "        line = lemmaSentence(frase)\n",
    "        listatweets.append(line)\n",
    "    return pd.Series(listatweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creo un preprocesador que utilizar√© posteriormente\n",
    "#En √©l trato de eliminar las urls, signos de puntuaci√≥n y stopwords\n",
    "#as√≠ como palabras como RT, posteriormente hago stemming\n",
    "def preProcessor(tweet):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    tweet = str(emoji_pattern.sub(r'', str(tweet))) # no emoji\n",
    "    pal = tknzr.tokenize(str(tweet))\n",
    "    urls = re.compile(r'.http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    line = urls.sub('', str(tweet))\n",
    "    ht = re.compile(r'http.')\n",
    "    bar = re.compile(r'//*')\n",
    "    punctuation = set(string.punctuation)\n",
    "    stoplist = stopwords.words('english')\n",
    "    pr = [\"rt\",\"@\",\"http\",\"https\",\"'s\",'...', 'english', 'translation','):', '. .', '..']\n",
    "    pal = [stemmer.stem(str(i)) for i in pal if i not in pr \n",
    "            if i not in stoplist if i not in punctuation\n",
    "            if not bar.search(i) if not ht.search(i)\n",
    "            if not i.isdigit() if not i.startswith('#')]\n",
    "    #pal = lemmaWord(pal)\n",
    "    tweet = pal\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "isis = pd.read_csv('isisfanboy.csv')\n",
    "about = pd.read_csv('aboutisis.csv')\n",
    "\n",
    "isis = isis[:17392]\n",
    "about = about[:17392]\n",
    "#El m√°ximo que puedo usar es 17392\n",
    "\n",
    "\n",
    "dataframe = pd.concat([isis, about])\n",
    "# Define X and Y\n",
    "X = dataframe['tweets'].values\n",
    "y = dataframe['radical'].values\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import cross_val_score, KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "from collections import Counter \n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "\n",
    "\n",
    "ngrams_featurizer = Pipeline([\n",
    "  ('count_vectorizer',  CountVectorizer(ngram_range = (1, 3), encoding = 'ISO-8859-1', \n",
    "                                        analyzer=preProcessor)),\n",
    "  ('tfidf_transformer', TfidfTransformer())\n",
    "])\n",
    "    \n",
    "    \n",
    "class PosStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Obtain number of tokens with POS categories\"\"\"\n",
    "\n",
    "    def stats(self, tweet):\n",
    "        tokens = tknzr.tokenize(str(tweet))\n",
    "        tagged = pos_tag(tokens, tagset='universal')\n",
    "        counts = Counter(tag for word,tag in tagged)\n",
    "        total = sum(counts.values())\n",
    "        #copy tags so that we return always the same number of features\n",
    "        pos_features = {'NOUN': 0, 'ADJ': 0, 'VERB': 0, 'ADV': 0, 'CONJ': 0, \n",
    "                        'ADP': 0, 'PRON':0, 'NUM': 0}\n",
    "        \n",
    "        pos_dic = dict((tag, float(count)/total) for tag,count in counts.items())\n",
    "        for k in pos_dic:\n",
    "            if k in pos_features:\n",
    "                pos_features[k] = pos_dic[k]\n",
    "        #print(pos_features)\n",
    "        return pos_features\n",
    "    \n",
    "    def transform(self, data, y=None):\n",
    "        print(\"Entra en Pos Stats**\")\n",
    "        dataproc = preProcess(data)\n",
    "        result = [self.stats(tweet) for tweet in dataproc]\n",
    "        return result\n",
    "    \n",
    "    def fit(self, data, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Hashtags(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    listwords=[]\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def notinlist(self, item, lista):\n",
    "        booleano = True\n",
    "        if item in lista:\n",
    "            booleano = False\n",
    "        return booleano    \n",
    "    \n",
    "    def allHashtags(self, tweet, lista):\n",
    "        lista = pickle.load( open( \"ht.p\", \"rb\" ) )\n",
    "        return lista\n",
    "    \n",
    "    def Hashtags(self, tweet, listallht, resultlist):\n",
    "        allhtdict = dict((ht, 0) for ht in listallht)\n",
    "        sent = tknzrwhu.tokenize(str(tweet))       \n",
    "\n",
    "        for term in sent:\n",
    "            if term in listallht:\n",
    "                allhtdict[term] = 1\n",
    "        resultlist.append(allhtdict)\n",
    "        \n",
    "        return (resultlist)\n",
    "\n",
    "\n",
    "    def transform(self, data):\n",
    "        print(\"Entra en hashtags\")\n",
    "        dataproc = preProcess(data)\n",
    "        lista = []\n",
    "        listaresultado = []\n",
    "        for tweet in dataproc:\n",
    "            lista = self.allHashtags(tweet, lista)\n",
    "        for tweet in dataproc:\n",
    "            listaresultado = self.Hashtags(tweet, lista, listaresultado)\n",
    "        return listaresultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "\n",
    "st = StanfordNERTagger('/usr/share/stanford-ner-2017-06-09/classifiers/english.all.3class.caseless.distsim.crf.ser.gz', \n",
    "                       '/usr/share/stanford-ner-2017-06-09/stanford-ner.jar', \n",
    "                      )\n",
    "\n",
    "st2 = StanfordNERTagger('/usr/share/stanford-ner-2017-06-09/classifiers/english.muc.7class.distsim.crf.ser.gz', \n",
    "                       '/usr/share/stanford-ner-2017-06-09/stanford-ner.jar', \n",
    "                       )\n",
    "\n",
    "\n",
    "class NameRecog(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def notinlist(self, item, lista):\n",
    "        booleano = True\n",
    "        if item in lista:\n",
    "            booleano = False\n",
    "        return booleano\n",
    "    \n",
    "    \n",
    "    def allner(self, dataproc, lista):\n",
    "        lista = pickle.load( open( \"ner.p\", \"rb\" ) )\n",
    "        return lista\n",
    " \n",
    "\n",
    "    def nameentrec(self, tweet, listallner, listasoloner, resultlist):\n",
    "        #Inicializo el diccionario siempre, todas las filas tendran los mismos elementos.\n",
    "        allnerdict = dict((entity, 0) for entity in listallner)\n",
    "        words = tknzrwhu.tokenize(str(tweet))\n",
    "        ne_tagged = []\n",
    "        b = False\n",
    "        \n",
    "        for i in words:\n",
    "            if i.lower() in listasoloner:\n",
    "                for k in allnerdict:\n",
    "                    if i.lower() == k[0]:\n",
    "                        allnerdict[k] = 1\n",
    "                        \n",
    "        resultlist.append(allnerdict)\n",
    "        return (resultlist)\n",
    "\n",
    "    def transform(self, data):\n",
    "        dataproc = preProcess(data)\n",
    "        print(\"Entra en NER\")\n",
    "        lista = []\n",
    "        listaresultado = []\n",
    "        listasolo = []\n",
    "        lista = self.allner(dataproc,lista)\n",
    "        for k in lista:\n",
    "            listasolo.append(k[0])\n",
    "        for tweet in dataproc:\n",
    "            listaresultado = self.nameentrec(tweet, lista, listasolo, listaresultado)\n",
    "\n",
    "        return listaresultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from preprocessor import preProcessSerie\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import pickle\n",
    "\n",
    "class Sentiment(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, data, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self, data, y=None):\n",
    "        print(\"Entra en Sentiments\")\n",
    "        sentdicc = pickle.load( open( \"sentdicc.p\", \"rb\" ) )\n",
    "        lista = []\n",
    "        for tweet in data:\n",
    "            tweetsent = sentdicc[tweet]\n",
    "            lista.append(tweetsent)\n",
    "        return lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelinesvm = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('words', TfidfVectorizer(encoding='utf-8', analyzer=preProcessor)),\n",
    "        ('ngrams', ngrams_featurizer),\n",
    "        ('pos_stats', Pipeline([\n",
    "                                ('pos_stats', PosStats()),\n",
    "                                ('vectors', DictVectorizer())\n",
    "                            ])),\n",
    "        ('ner', Pipeline([\n",
    "            ('nameentrec', NameRecog()),\n",
    "            ('vectors', DictVectorizer())\n",
    "        ])),\n",
    "        ('hashtags', Pipeline([\n",
    "            ('gethashtags', Hashtags()),\n",
    "            ('vect', DictVectorizer())\n",
    "        ])),\n",
    "        ('sentiments', Pipeline([\n",
    "            ('getsentiments', Sentiment()),\n",
    "            ('vector', TfidfVectorizer(encoding='utf-8'))\n",
    "        ]))\n",
    "    ])),\n",
    "\n",
    "    ('clf', SVC(C=10, gamma= 1, kernel='rbf', probability=True)\n",
    ")\n",
    "\n",
    "])\n",
    "\n",
    "#cv = KFold(X.shape[0], 4, shuffle=True, random_state=33)\n",
    "#scores = cross_val_score(pipeline, X, y, cv=cv)\n",
    "#print(\"Scores in every iteration\", scores)\n",
    "#print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "pipelinesvm.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isisnum = pd.read_csv('isisfanboynum.csv')\n",
    "aboutnum = pd.read_csv('aboutisisnum.csv')\n",
    "\n",
    "isisnum = isisnum[:17392]\n",
    "aboutnum = aboutnum[:17392]\n",
    "#El m√°ximo que puedo usar es 17392\n",
    "\n",
    "\n",
    "dataframenum = pd.concat([isisnum, aboutnum])\n",
    "# Define X and Y\n",
    "Xnum = dataframenum['tweets'].values\n",
    "ynum = dataframenum['radical'].values\n",
    "\n",
    "print(np.unique(ynum))\n",
    "pipelinesvmwordsnum = Pipeline([\n",
    "    ('words', TfidfVectorizer(encoding='utf-8', analyzer=preProcessor)),\n",
    "    ('clf', SVC(C=10, gamma= 1, kernel='rbf', probability=True))\n",
    "\n",
    "])\n",
    "print(\"Hola, ha entrado\")\n",
    "cv = KFold(X.shape[0], 10, shuffle=True, random_state=33)\n",
    "scoressvmwordsnum = cross_val_score(pipelinesvmwordsnum, Xnum, ynum, cv=cv)\n",
    "print(\"Scores in every iteration\", scoressvmwordsnum)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scoressvmwordsnum.mean(), scoressvmwordsnum.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipelinekpos = Pipeline([\n",
    "    ('pos_stats', Pipeline([\n",
    "        ('pos_stats', PosStats()),\n",
    "        ('vectors', DictVectorizer())\n",
    "    ])),\n",
    "    ('modelknn', KNeighborsClassifier(n_neighbors = 13))\n",
    "\n",
    "])\n",
    "\n",
    "cv = KFold(X.shape[0], 10, shuffle=True, random_state=33)\n",
    "scoreskpos = cross_val_score(pipelinekpos, X, y, cv=cv)\n",
    "print(\"Scores in every iteration\", scoreskpos)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scoreskpos.mean(), scoreskpos.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelinenbht = Pipeline([\n",
    "    ('hashtags', Pipeline([\n",
    "        ('gethashtags', Hashtags()),\n",
    "        ('vect', DictVectorizer())\n",
    "    ])),\n",
    "    ('clf', AdaBoostClassifier(n_estimators=195, base_estimator=MultinomialNB(alpha=.01), learning_rate=1))\n",
    "\n",
    "])\n",
    "\n",
    "cv = KFold(X.shape[0], 10, shuffle=True, random_state=33)\n",
    "scoresnbht = cross_val_score(pipelinenbht, X, y, cv=cv)\n",
    "print(\"Scores in every iteration\", scoresnbht)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scoresnbht.mean(), scoresnbht.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipelinesvmht = Pipeline([\n",
    "    ('hashtags', Pipeline([\n",
    "        ('gethashtags', Hashtags()),\n",
    "        ('vect', DictVectorizer())\n",
    "    ])),\n",
    "    ('clf', SVC(C=10, gamma= 1, kernel='rbf', probability=True))\n",
    "\n",
    "])\n",
    "\n",
    "cv = KFold(X.shape[0], 10, shuffle=True, random_state=33)\n",
    "scoressvmht = cross_val_score(pipelinesvmht, X, y, cv=cv)\n",
    "print(\"Scores in every iteration\", scoressvmht)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scoressvmht.mean(), scoressvmht.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipelinenbsent = Pipeline([\n",
    "    ('sentiments', Pipeline([\n",
    "        ('getsentiments', Sentiment()),\n",
    "        ('vector', TfidfVectorizer(encoding='utf-8'))\n",
    "        ]))\n",
    "    ('clf', AdaBoostClassifier(n_estimators=195, base_estimator=MultinomialNB(alpha=.01), learning_rate=1))\n",
    "\n",
    "])\n",
    "\n",
    "cv = KFold(X.shape[0], 10, shuffle=True, random_state=33)\n",
    "scoresnbsent = cross_val_score(pipelinenbsent, X, y, cv=cv)\n",
    "print(\"Scores in every iteration\", scoresnbsent)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scoresnbsent.mean(), scoresnbsent.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipelinesvmsent = Pipeline([\n",
    "    ('sentiments', Pipeline([\n",
    "        ('getsentiments', Sentiment()),\n",
    "        ('vector', TfidfVectorizer(encoding='utf-8'))\n",
    "    ]))\n",
    "    ('clf', SVC(C=10, gamma= 1, kernel='rbf', probability=True))\n",
    "\n",
    "])\n",
    "\n",
    "cv = KFold(X.shape[0], 10, shuffle=True, random_state=33)\n",
    "scoressvmsent = cross_val_score(pipelinesvmsent, X, y, cv=cv)\n",
    "print(\"Scores in every iteration\", scoressvmsent)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scoressvmsent.mean(), scoressvmsent.std() * 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
